{
 "metadata": {
  "name": "",
  "signature": "sha256:febb933b1d86368ed7adc64e5fa4746564e2ac037bff9c8d8df19a271dcc096f"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Generating the training and test set involves using the `ocbio.extract` module with the chosen gold standard positive and negative datasets.\n",
      "This notebook is supposed to act like a script to do this, with documentation inline.\n",
      "\n",
      "First, the datasource table must be regenerated at the _top directory_ containing the data:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "cd ../../"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "/data/opencast/MRes\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import csv"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "As the data repository has now been annexed the datasource table must first be unlocked:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!git annex unlock datasource.tab"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "unlock datasource.tab (copying...) ok\r\n"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#this script should be updated to add new features when available\n",
      "f = open(\"datasource.tab\", \"w\")\n",
      "c = csv.writer(f,delimiter=\"\\t\")\n",
      "# the HIPPIE feature\n",
      "c.writerow([\"HIPPIE/hippie_current.txt\",\"HIPPIE/feature.HIPPIE.db\",\n",
      "            \"protindexes=(1,3);valindexes=(4);zeromissing=1\"])\n",
      "# the abundance feature\n",
      "c.writerow([\"forGAVIN/pulldown_data/dataset/ppi_ab_entrez.csv\",\n",
      "        \"forGAVIN/pulldown_data/dataset/abundance.Entrez.db\",\n",
      "        \"ignoreheader=1;zeromissinginternal=1;fillmissing=forGAVIN/pulldown_data/dataset/abundance.average.pickle\"])\n",
      "# the affinity feature\n",
      "c.writerow([\"affinityresults/results2/unique_data_ppi_coor_C2S_entrez.csv\",\n",
      "        \"affinityresults/results2/affinity.Entrez.db\",\n",
      "        \"zeromissinginternal=1;fillmissing=affinityresults/results2/affinity.pulldown.average.pickle\"])\n",
      "# Gene Ontology features\n",
      "c.writerow([\"Gene_Ontology\",\"Gene_Ontology\",\"generator=geneontology/testgen.pickle\"])\n",
      "# Y2H SVM feature\n",
      "c.writerow([\"Y2H/Y2H.txt\",\"Y2H/Y2H.db\",\"valindexes=(4);ignoreheader=1;zeromissing=1\"])\n",
      "# ENTS feature\n",
      "c.writerow([\"ENTS\",\"ENTS\",\"generator=ents/human.ENTS.features.pickle\"])\n",
      "# iRefIndex features\n",
      "c.writerow([\"iRefIndex\",\"iRefIndex\",\"generator=iRefIndex/human.iRefIndex.Entrez.1ofk.pickle\"])\n",
      "# STRING features\n",
      "c.writerow([\"STRING\",\"STRING\",\"generator=string/human.Entrez.string.pickle\"])\n",
      "# ENTS summary feature\n",
      "c.writerow([\"ENTS_summary\",\"ENTS_summary\",\"generator=ents/human.Entrez.ENTS.summary.pickle\"])\n",
      "# HMR feature\n",
      "c.writerow([\"HMR\",\"HMR\",\"generator=HMR/human.HMR.features.pickle\"])\n",
      "# InterologWalk feature\n",
      "c.writerow([\"InterologWalk\",\"InterologWalk\",\n",
      "            \"generator=InterologWalk/human.interologwalk.features.pickle\"])\n",
      "f.close()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Importing ocbio.extract\n",
      "\n",
      "Next, `ocbio.extract` must be added to the path and imported:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import sys"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sys.path.append(\"opencast-bio/\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import ocbio.extract"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "reload(ocbio.extract)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 7,
       "text": [
        "<module 'ocbio.extract' from 'opencast-bio/ocbio/extract.pyc'>"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Unlocking databases\n",
      "\n",
      "Now that the data directory has been [annexed][gitannex] the database files must first be unlocked:\n",
      "\n",
      "[gitannex]: https://git-annex.branchable.com/"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!git annex unlock Y2H/Y2H.db"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!git annex unlock affinityresults/results2/affinity.Entrez.db"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!git annex unlock forGAVIN/pulldown_data/dataset/abundance.Entrez.db"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!git annex unlock HIPPIE/feature.HIPPIE.db"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Initialising assembler\n",
      "\n",
      "Then an assembler object must be initialised using the data source table:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "assembler = ocbio.extract.FeatureVectorAssembler(\"datasource.tab\", verbose=True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Using  from top data directory datasource.tab.\n",
        "Reading data source table:\n",
        "\tData source: HIPPIE/hippie_current.txt to be processed to HIPPIE/feature.HIPPIE.db\n",
        "\tData source: forGAVIN/pulldown_data/dataset/ppi_ab_entrez.csv to be processed to forGAVIN/pulldown_data/dataset/abundance.Entrez.db\n",
        "\tData source: affinityresults/results2/unique_data_ppi_coor_C2S_entrez.csv to be processed to affinityresults/results2/affinity.Entrez.db\n",
        "\tData source: Gene_Ontology to be processed to Gene_Ontology\n",
        "\tData source: Y2H/Y2H.txt to be processed to Y2H/Y2H.db\n",
        "\tData source: ENTS to be processed to ENTS\n",
        "\tData source: iRefIndex to be processed to iRefIndex\n",
        "\tData source: STRING to be processed to STRING\n",
        "\tData source: ENTS_summary to be processed to ENTS_summary\n",
        "\tData source: HMR to be processed to HMR\n",
        "\tData source: InterologWalk to be processed to InterologWalk\n",
        "Initialising parsers.\n",
        "Database HIPPIE/feature.HIPPIE.db last updated 2014-06-25 12:04:07\n",
        "Database forGAVIN/pulldown_data/dataset/abundance.Entrez.db last updated 2014-06-25 12:04:08"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Database affinityresults/results2/affinity.Entrez.db last updated 2014-06-25 12:06:47"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Database Y2H/Y2H.db last updated 2014-06-25 12:15:04"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Finished Initialisation."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Regenerating features\n",
      "\n",
      "Then all the features should be regenerated to ensure they are up to date:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "assembler.regenerate(verbose=True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Regenerating parsers:\n",
        "\t parser 0\n",
        "Database HIPPIE/feature.HIPPIE.db last updated 2014-06-25 12:04:07"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\t parser 1\n",
        "Database forGAVIN/pulldown_data/dataset/abundance.Entrez.db last updated 2014-06-25 12:04:08"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\t parser 2\n",
        "Database affinityresults/results2/affinity.Entrez.db last updated 2014-06-25 12:06:47\n",
        "\t parser 3\n",
        "Custom generator function, no database to regenerate.\n",
        "\t parser 4\n",
        "Database Y2H/Y2H.db last updated 2014-06-25 12:15:04"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\t parser 5\n",
        "Custom generator function, no database to regenerate.\n",
        "\t parser 6\n",
        "Custom generator function, no database to regenerate.\n",
        "\t parser 7\n",
        "Custom generator function, no database to regenerate.\n"
       ]
      }
     ],
     "prompt_number": 21
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Writing DIP training set\n",
      "\n",
      "Then the assemble command can be used with the two DIP gold standard flat files:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!git annex unlock features/training.nolabel.positive.Entrez.vectors.txt"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "unlock features/training.nolabel.positive.Entrez.vectors.txt (copying...) "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "ok\r\n"
       ]
      }
     ],
     "prompt_number": 14
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "assembler.assemble(\"DIP/human/training.nolabel.positive.Entrez.txt\",\n",
      "                   \"features/training.nolabel.positive.Entrez.vectors.txt\", verbose=True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Reading pairfile: DIP/human/training.nolabel.positive.Entrez.txt\n",
        "Checking feature sizes:\n",
        "\t Data source HIPPIE/hippie_current.txt produces features of size 1.\n",
        "\t Data source forGAVIN/pulldown_data/dataset/ppi_ab_entrez.csv produces features of size 1.\n",
        "\t Data source affinityresults/results2/unique_data_ppi_coor_C2S_entrez.csv produces features of size 1.\n",
        "\t Data source Gene_Ontology produces features of size 90.\n",
        "\t Data source Y2H/Y2H.txt produces features of size 1.\n",
        "\t Data source ENTS produces features of size 107.\n",
        "\t Data source iRefIndex produces features of size 11.\n",
        "\t Data source STRING produces features of size 8.\n",
        "\t Data source ENTS_summary produces features of size 1.\n",
        "\t Data source HMR produces features of size 1.\n",
        "\t Data source InterologWalk produces features of size 1.\n",
        "Writing feature vectors\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Wrote 4857 vectors.\n",
        "Matched 100.00 % of protein pairs in DIP/human/training.nolabel.positive.Entrez.txt to features from HIPPIE/hippie_current.txt\n",
        "Matched 100.00 % of protein pairs in DIP/human/training.nolabel.positive.Entrez.txt to features from forGAVIN/pulldown_data/dataset/ppi_ab_entrez.csv\n",
        "Matched 100.00 % of protein pairs in DIP/human/training.nolabel.positive.Entrez.txt to features from affinityresults/results2/unique_data_ppi_coor_C2S_entrez.csv\n",
        "Matched 100.00 % of protein pairs in DIP/human/training.nolabel.positive.Entrez.txt to features from Gene_Ontology\n",
        "Matched 100.00 % of protein pairs in DIP/human/training.nolabel.positive.Entrez.txt to features from Y2H/Y2H.txt\n",
        "Matched 35.00 % of protein pairs in DIP/human/training.nolabel.positive.Entrez.txt to features from ENTS\n",
        "Matched 100.00 % of protein pairs in DIP/human/training.nolabel.positive.Entrez.txt to features from iRefIndex\n",
        "Matched 100.00 % of protein pairs in DIP/human/training.nolabel.positive.Entrez.txt to features from STRING\n",
        "Matched 100.00 % of protein pairs in DIP/human/training.nolabel.positive.Entrez.txt to features from ENTS_summary\n",
        "Matched 100.00 % of protein pairs in DIP/human/training.nolabel.positive.Entrez.txt to features from HMR\n",
        "Matched 100.00 % of protein pairs in DIP/human/training.nolabel.positive.Entrez.txt to features from InterologWalk\n"
       ]
      }
     ],
     "prompt_number": 18
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!git annex unlock features/training.nolabel.negative.Entrez.vectors.txt"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "unlock features/training.nolabel.negative.Entrez.vectors.txt (copying...) "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "ok\r\n"
       ]
      }
     ],
     "prompt_number": 19
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "assembler.assemble(\"DIP/human/training.nolabel.negative.Entrez.txt\",\n",
      "                   \"features/training.nolabel.negative.Entrez.vectors.txt\", verbose=True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Reading pairfile: DIP/human/training.nolabel.negative.Entrez.txt\n",
        "Checking feature sizes:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\t Data source HIPPIE/hippie_current.txt produces features of size 1.\n",
        "\t Data source forGAVIN/pulldown_data/dataset/ppi_ab_entrez.csv produces features of size 1.\n",
        "\t Data source affinityresults/results2/unique_data_ppi_coor_C2S_entrez.csv produces features of size 1.\n",
        "\t Data source Gene_Ontology produces features of size 90.\n",
        "\t Data source Y2H/Y2H.txt produces features of size 1.\n",
        "\t Data source ENTS produces features of size 107.\n",
        "\t Data source iRefIndex produces features of size 11.\n",
        "\t Data source STRING produces features of size 8.\n",
        "\t Data source ENTS_summary produces features of size 1.\n",
        "\t Data source HMR produces features of size 1.\n",
        "\t Data source InterologWalk produces features of size 1.\n",
        "Writing feature vectors."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Wrote 3061800 vectors.\n",
        "Matched 100.00 % of protein pairs in DIP/human/training.nolabel.negative.Entrez.txt to features from HIPPIE/hippie_current.txt\n",
        "Matched 100.00 % of protein pairs in DIP/human/training.nolabel.negative.Entrez.txt to features from forGAVIN/pulldown_data/dataset/ppi_ab_entrez.csv\n",
        "Matched 100.00 % of protein pairs in DIP/human/training.nolabel.negative.Entrez.txt to features from affinityresults/results2/unique_data_ppi_coor_C2S_entrez.csv\n",
        "Matched 100.00 % of protein pairs in DIP/human/training.nolabel.negative.Entrez.txt to features from Gene_Ontology\n",
        "Matched 100.00 % of protein pairs in DIP/human/training.nolabel.negative.Entrez.txt to features from Y2H/Y2H.txt\n",
        "Matched 29.23 % of protein pairs in DIP/human/training.nolabel.negative.Entrez.txt to features from ENTS\n",
        "Matched 100.00 % of protein pairs in DIP/human/training.nolabel.negative.Entrez.txt to features from iRefIndex\n",
        "Matched 100.00 % of protein pairs in DIP/human/training.nolabel.negative.Entrez.txt to features from STRING\n",
        "Matched 100.00 % of protein pairs in DIP/human/training.nolabel.negative.Entrez.txt to features from ENTS_summary\n",
        "Matched 100.00 % of protein pairs in DIP/human/training.nolabel.negative.Entrez.txt to features from HMR\n",
        "Matched 100.00 % of protein pairs in DIP/human/training.nolabel.negative.Entrez.txt to features from InterologWalk\n"
       ]
      }
     ],
     "prompt_number": 20
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Writing pulldown vectors\n",
      "\n",
      "These feature vectors correspond to all possible protein pairs involved in the pulldown experiment."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!git annex unlock features/pulldown.DIP.Entrez.vectors.txt"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "unlock features/pulldown.DIP.Entrez.vectors.txt (copying...) "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "ok\r\n"
       ]
      }
     ],
     "prompt_number": 21
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "assembler.assemble(\"forGAVIN/pulldown_data/pulldown.interactions.Entrez.tsv\",\n",
      "                   \"features/pulldown.DIP.Entrez.vectors.txt\", verbose=True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Reading pairfile: forGAVIN/pulldown_data/pulldown.interactions.Entrez.tsv\n",
        "Checking feature sizes:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\t Data source HIPPIE/hippie_current.txt produces features of size 1.\n",
        "\t Data source forGAVIN/pulldown_data/dataset/ppi_ab_entrez.csv produces features of size 1.\n",
        "\t Data source affinityresults/results2/unique_data_ppi_coor_C2S_entrez.csv produces features of size 1.\n",
        "\t Data source Gene_Ontology produces features of size 90.\n",
        "\t Data source Y2H/Y2H.txt produces features of size 1.\n",
        "\t Data source ENTS produces features of size 107.\n",
        "\t Data source iRefIndex produces features of size 11.\n",
        "\t Data source STRING produces features of size 8.\n",
        "\t Data source ENTS_summary produces features of size 1.\n",
        "\t Data source HMR produces features of size 1.\n",
        "\t Data source InterologWalk produces features of size 1.\n",
        "Writing feature vectors."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Wrote 1699246 vectors.\n",
        "Matched 100.00 % of protein pairs in forGAVIN/pulldown_data/pulldown.interactions.Entrez.tsv to features from HIPPIE/hippie_current.txt\n",
        "Matched 100.00 % of protein pairs in forGAVIN/pulldown_data/pulldown.interactions.Entrez.tsv to features from forGAVIN/pulldown_data/dataset/ppi_ab_entrez.csv\n",
        "Matched 100.00 % of protein pairs in forGAVIN/pulldown_data/pulldown.interactions.Entrez.tsv to features from affinityresults/results2/unique_data_ppi_coor_C2S_entrez.csv\n",
        "Matched 100.00 % of protein pairs in forGAVIN/pulldown_data/pulldown.interactions.Entrez.tsv to features from Gene_Ontology\n",
        "Matched 100.00 % of protein pairs in forGAVIN/pulldown_data/pulldown.interactions.Entrez.tsv to features from Y2H/Y2H.txt\n",
        "Matched 44.05 % of protein pairs in forGAVIN/pulldown_data/pulldown.interactions.Entrez.tsv to features from ENTS\n",
        "Matched 100.00 % of protein pairs in forGAVIN/pulldown_data/pulldown.interactions.Entrez.tsv to features from iRefIndex\n",
        "Matched 100.00 % of protein pairs in forGAVIN/pulldown_data/pulldown.interactions.Entrez.tsv to features from STRING\n",
        "Matched 100.00 % of protein pairs in forGAVIN/pulldown_data/pulldown.interactions.Entrez.tsv to features from ENTS_summary\n",
        "Matched 100.00 % of protein pairs in forGAVIN/pulldown_data/pulldown.interactions.Entrez.tsv to features from HMR\n",
        "Matched 100.00 % of protein pairs in forGAVIN/pulldown_data/pulldown.interactions.Entrez.tsv to features from InterologWalk\n"
       ]
      }
     ],
     "prompt_number": 22
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Writing 50% HIPPIE training set\n",
      "\n",
      "We are also planning to build a model using HIPPIE as the training set.\n",
      "To do this we will have to write corresponding feature vectors for each protein pair in the HIPPIE database with a confidence value above 0.5.\n",
      "The HIPPIE feature must be removed from the assembler first, though."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print assembler.parserlist[0].datadir"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "HIPPIE/hippie_current.txt\n"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "del assembler.parserlist[0]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!git annex unlock features/training.HIPPIE.positive.Entrez.vectors.txt"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 14
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "assembler.assemble(\"HIPPIE/hippie_current.pairs.txt\",\n",
      "                   \"features/training.HIPPIE.positive.50.Entrez.vectors.txt\", verbose=True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Reading pairfile: HIPPIE/hippie_current.pairs.txt\n",
        "Checking feature sizes:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\t Data source forGAVIN/pulldown_data/dataset/ppi_ab_entrez.csv produces features of size 1.\n",
        "\t Data source affinityresults/results2/unique_data_ppi_coor_C2S_entrez.csv produces features of size 1.\n",
        "\t Data source Gene_Ontology produces features of size 90.\n",
        "\t Data source Y2H/Y2H.txt produces features of size 1.\n",
        "\t Data source ENTS produces features of size 107.\n",
        "\t Data source iRefIndex produces features of size 11.\n",
        "\t Data source STRING produces features of size 8.\n",
        "\t Data source ENTS_summary produces features of size 1.\n",
        "\t Data source HMR produces features of size 1.\n",
        "\t Data source InterologWalk produces features of size 1.\n",
        "Writing feature vectors."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Wrote 169626 vectors.\n",
        "Matched 100.00 % of protein pairs in HIPPIE/hippie_current.pairs.txt to features from forGAVIN/pulldown_data/dataset/ppi_ab_entrez.csv\n",
        "Matched 100.00 % of protein pairs in HIPPIE/hippie_current.pairs.txt to features from affinityresults/results2/unique_data_ppi_coor_C2S_entrez.csv\n",
        "Matched 100.00 % of protein pairs in HIPPIE/hippie_current.pairs.txt to features from Gene_Ontology\n",
        "Matched 100.00 % of protein pairs in HIPPIE/hippie_current.pairs.txt to features from Y2H/Y2H.txt\n",
        "Matched 44.64 % of protein pairs in HIPPIE/hippie_current.pairs.txt to features from ENTS\n",
        "Matched 100.00 % of protein pairs in HIPPIE/hippie_current.pairs.txt to features from iRefIndex\n",
        "Matched 100.00 % of protein pairs in HIPPIE/hippie_current.pairs.txt to features from STRING\n",
        "Matched 100.00 % of protein pairs in HIPPIE/hippie_current.pairs.txt to features from ENTS_summary\n",
        "Matched 100.00 % of protein pairs in HIPPIE/hippie_current.pairs.txt to features from HMR\n",
        "Matched 100.00 % of protein pairs in HIPPIE/hippie_current.pairs.txt to features from InterologWalk\n"
       ]
      }
     ],
     "prompt_number": 15
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Since these interactions are not all high confidence it's uncertain how many negative interactions we should have as training data.\n",
      "In lieu of a better solution (and because 101 million negative interactions would be intractable) we will just use the negative interaction list generated for DIP:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!git annex unlock features/training.HIPPIE.negative.Entrez.vectors.txt"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "unlock features/training.HIPPIE.negative.Entrez.vectors.txt (copying...) "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "ok\r\n"
       ]
      }
     ],
     "prompt_number": 29
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "assembler.assemble(\"DIP/human/training.nolabel.negative.Entrez.txt\",\n",
      "                   \"features/training.HIPPIE.negative.Entrez.vectors.txt\", verbose=True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Reading pairfile: DIP/human/training.nolabel.negative.Entrez.txt\n",
        "Checking feature sizes:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\t Data source forGAVIN/pulldown_data/dataset/ppi_ab_entrez.csv produces features of size 1.\n",
        "\t Data source affinityresults/results2/unique_data_ppi_coor_C2S_entrez.csv produces features of size 1.\n",
        "\t Data source Gene_Ontology produces features of size 90.\n",
        "\t Data source Y2H/Y2H.txt produces features of size 1.\n",
        "\t Data source ENTS produces features of size 107.\n",
        "\t Data source iRefIndex produces features of size 11.\n",
        "\t Data source STRING produces features of size 8.\n",
        "\t Data source ENTS_summary produces features of size 1.\n",
        "\t Data source HMR produces features of size 1.\n",
        "\t Data source InterologWalk produces features of size 1.\n",
        "Writing feature vectors."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Wrote 3061800 vectors.\n",
        "Matched 100.00 % of protein pairs in DIP/human/training.nolabel.negative.Entrez.txt to features from forGAVIN/pulldown_data/dataset/ppi_ab_entrez.csv\n",
        "Matched 100.00 % of protein pairs in DIP/human/training.nolabel.negative.Entrez.txt to features from affinityresults/results2/unique_data_ppi_coor_C2S_entrez.csv\n",
        "Matched 100.00 % of protein pairs in DIP/human/training.nolabel.negative.Entrez.txt to features from Gene_Ontology\n",
        "Matched 100.00 % of protein pairs in DIP/human/training.nolabel.negative.Entrez.txt to features from Y2H/Y2H.txt\n",
        "Matched 29.23 % of protein pairs in DIP/human/training.nolabel.negative.Entrez.txt to features from ENTS\n",
        "Matched 100.00 % of protein pairs in DIP/human/training.nolabel.negative.Entrez.txt to features from iRefIndex\n",
        "Matched 100.00 % of protein pairs in DIP/human/training.nolabel.negative.Entrez.txt to features from STRING\n",
        "Matched 100.00 % of protein pairs in DIP/human/training.nolabel.negative.Entrez.txt to features from ENTS_summary\n",
        "Matched 100.00 % of protein pairs in DIP/human/training.nolabel.negative.Entrez.txt to features from HMR\n",
        "Matched 100.00 % of protein pairs in DIP/human/training.nolabel.negative.Entrez.txt to features from InterologWalk\n"
       ]
      }
     ],
     "prompt_number": 30
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We should check if this set of negative example pairs is still valid by making sure it does not overlap with the positive set:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "f = open(\"HIPPIE/hippie_current.pairs.txt\")\n",
      "c = csv.reader(f,delimiter=\"\\t\")\n",
      "pospairs = []\n",
      "for l in c:\n",
      "    pospairs.append(frozenset(l))\n",
      "f.close()\n",
      "pospairs = set(pospairs)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 17
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "f = open(\"DIP/human/training.nolabel.negative.Entrez.txt\")\n",
      "c = csv.reader(f,delimiter=\"\\t\")\n",
      "conflicts = {}\n",
      "for i,l in enumerate(c):\n",
      "    if frozenset(l) in pospairs:\n",
      "        conflicts[i] = frozenset(l)\n",
      "print \"{0} conflicts detected.\".format(len(conflicts.keys()))\n",
      "f.close()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "21278 conflicts detected.\n"
       ]
      }
     ],
     "prompt_number": 20
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "To fix this problem quickly, we will simply pickle this dictionary and remove these entries from the training set in the notebook on [Classifier training][classtrain].\n",
      "\n",
      "[classtrain]: http://nbviewer.ipython.org/github/ggray1729/opencast-bio/blob/master/notebooks/Classifier%20Training%20HIPPIE.ipynb"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pickle"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 21
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "f = open(\"features/training.HIPPIE.negative.conflicts.pickle\",\"wb\")\n",
      "pickle.dump(conflicts,f)\n",
      "f.close()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 22
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## 90% HIPPIE training set\n",
      "\n",
      "Using the saved file containing only protein pairs above 90% in HIPPIE's confidence values.\n",
      "The negative interaction file from above will still be acceptable:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "assembler.assemble(\"HIPPIE/hippie_current.90.pairs.txt\",\n",
      "                   \"features/training.HIPPIE.positive.90.Entrez.vectors.txt\", verbose=True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Reading pairfile: HIPPIE/hippie_current.90.pairs.txt\n",
        "Checking feature sizes:\n",
        "\t Data source forGAVIN/pulldown_data/dataset/ppi_ab_entrez.csv produces features of size 1.\n",
        "\t Data source affinityresults/results2/unique_data_ppi_coor_C2S_entrez.csv produces features of size 1.\n",
        "\t Data source Gene_Ontology produces features of size 90.\n",
        "\t Data source Y2H/Y2H.txt produces features of size 1.\n",
        "\t Data source ENTS produces features of size 107."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\t Data source iRefIndex produces features of size 11.\n",
        "\t Data source STRING produces features of size 8.\n",
        "\t Data source ENTS_summary produces features of size 1.\n",
        "\t Data source HMR produces features of size 1.\n",
        "\t Data source InterologWalk produces features of size 1.\n",
        "Writing feature vectors\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Wrote 2451 vectors.\n",
        "Matched 100.00 % of protein pairs in HIPPIE/hippie_current.90.pairs.txt to features from forGAVIN/pulldown_data/dataset/ppi_ab_entrez.csv\n",
        "Matched 100.00 % of protein pairs in HIPPIE/hippie_current.90.pairs.txt to features from affinityresults/results2/unique_data_ppi_coor_C2S_entrez.csv\n",
        "Matched 100.00 % of protein pairs in HIPPIE/hippie_current.90.pairs.txt to features from Gene_Ontology\n",
        "Matched 100.00 % of protein pairs in HIPPIE/hippie_current.90.pairs.txt to features from Y2H/Y2H.txt\n",
        "Matched 46.19 % of protein pairs in HIPPIE/hippie_current.90.pairs.txt to features from ENTS\n",
        "Matched 100.00 % of protein pairs in HIPPIE/hippie_current.90.pairs.txt to features from iRefIndex\n",
        "Matched 100.00 % of protein pairs in HIPPIE/hippie_current.90.pairs.txt to features from STRING\n",
        "Matched 100.00 % of protein pairs in HIPPIE/hippie_current.90.pairs.txt to features from ENTS_summary\n",
        "Matched 100.00 % of protein pairs in HIPPIE/hippie_current.90.pairs.txt to features from HMR\n",
        "Matched 100.00 % of protein pairs in HIPPIE/hippie_current.90.pairs.txt to features from InterologWalk\n"
       ]
      }
     ],
     "prompt_number": 31
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Writing Pulldown feature vectors"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!git annex unlock features/pulldown.HIPPIE.Entrez.vectors.txt"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "unlock features/pulldown.HIPPIE.Entrez.vectors.txt (copying...) "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "ok\r\n"
       ]
      }
     ],
     "prompt_number": 32
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "assembler.assemble(\"forGAVIN/pulldown_data/pulldown.interactions.Entrez.tsv\",\n",
      "                   \"features/pulldown.HIPPIE.Entrez.vectors.txt\", verbose=True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Reading pairfile: forGAVIN/pulldown_data/pulldown.interactions.Entrez.tsv\n",
        "Checking feature sizes:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\t Data source forGAVIN/pulldown_data/dataset/ppi_ab_entrez.csv produces features of size 1.\n",
        "\t Data source affinityresults/results2/unique_data_ppi_coor_C2S_entrez.csv produces features of size 1.\n",
        "\t Data source Gene_Ontology produces features of size 90.\n",
        "\t Data source Y2H/Y2H.txt produces features of size 1.\n",
        "\t Data source ENTS produces features of size 107.\n",
        "\t Data source iRefIndex produces features of size 11.\n",
        "\t Data source STRING produces features of size 8.\n",
        "\t Data source ENTS_summary produces features of size 1.\n",
        "\t Data source HMR produces features of size 1.\n",
        "\t Data source InterologWalk produces features of size 1.\n",
        "Writing feature vectors."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Wrote 1699246 vectors.\n",
        "Matched 100.00 % of protein pairs in forGAVIN/pulldown_data/pulldown.interactions.Entrez.tsv to features from forGAVIN/pulldown_data/dataset/ppi_ab_entrez.csv\n",
        "Matched 100.00 % of protein pairs in forGAVIN/pulldown_data/pulldown.interactions.Entrez.tsv to features from affinityresults/results2/unique_data_ppi_coor_C2S_entrez.csv\n",
        "Matched 100.00 % of protein pairs in forGAVIN/pulldown_data/pulldown.interactions.Entrez.tsv to features from Gene_Ontology\n",
        "Matched 100.00 % of protein pairs in forGAVIN/pulldown_data/pulldown.interactions.Entrez.tsv to features from Y2H/Y2H.txt\n",
        "Matched 44.05 % of protein pairs in forGAVIN/pulldown_data/pulldown.interactions.Entrez.tsv to features from ENTS\n",
        "Matched 100.00 % of protein pairs in forGAVIN/pulldown_data/pulldown.interactions.Entrez.tsv to features from iRefIndex\n",
        "Matched 100.00 % of protein pairs in forGAVIN/pulldown_data/pulldown.interactions.Entrez.tsv to features from STRING\n",
        "Matched 100.00 % of protein pairs in forGAVIN/pulldown_data/pulldown.interactions.Entrez.tsv to features from ENTS_summary\n",
        "Matched 100.00 % of protein pairs in forGAVIN/pulldown_data/pulldown.interactions.Entrez.tsv to features from HMR\n",
        "Matched 100.00 % of protein pairs in forGAVIN/pulldown_data/pulldown.interactions.Entrez.tsv to features from InterologWalk\n"
       ]
      }
     ],
     "prompt_number": 33
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "assembler.close()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 13
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Indexing pulldown training set\n",
      "\n",
      "The two features extracted from pulldown sources, [abundance][] and [affinity][] scores, have poor coverage over the full training set.\n",
      "The chosen solution to this problem has been to fill the missing entries with a mean value and then train two classifiers - one on the full training set with a large number of training samples and one on the reduced training set of only the small number of samples.\n",
      "To do this, we need to know what indexes the pulldown interactions are in the full training set.\n",
      "\n",
      "First, we will load in the IDs of all the pulldown proteins:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "ls forGAVIN/pulldown_data/"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\u001b[0m\u001b[01;34mBAITS\u001b[0m/  \u001b[01;34mdataset\u001b[0m/  \u001b[01;34mPREYS\u001b[0m/  \u001b[01;36mpulldown.interactions.Entrez.tsv\u001b[0m@\r\n"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "baits = loadtxt(\"forGAVIN/pulldown_data/BAITS/baits_entrez_ids.csv\", dtype=str)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "preys = loadtxt(\"forGAVIN/pulldown_data/PREYS/prey_entrez_ids.csv\",dtype=str)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pulldownids = set(list(baits)+list(preys))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 13
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "f,fw = open(\"HIPPIE/hippie_current.pairs.txt\"), open(\"features/hippie.positive.pulldownindexes.txt\",\"w\")\n",
      "c,cw = csv.reader(f,delimiter=\"\\t\"), csv.writer(fw,delimiter=\"\\t\")\n",
      "for i,l in enumerate(c):\n",
      "    if l[0] in pulldownids and l[1] in pulldownids:\n",
      "        cw.writerow([i])\n",
      "f.close()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 21
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "f,fw = open(\"DIP/human/training.nolabel.negative.Entrez.txt\"), open(\"features/hippie.negative.pulldownindexes.txt\",\"w\")\n",
      "c,cw = csv.reader(f,delimiter=\"\\t\"), csv.writer(fw,delimiter=\"\\t\")\n",
      "for i,l in enumerate(c):\n",
      "    if l[0] in pulldownids and l[1] in pulldownids:\n",
      "        cw.writerow([i])\n",
      "f.close()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 24
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Writing reduced pulldown feature set\n",
      "\n",
      "A reduced version of the pulldown network with interactions of interest already identified is available at Edinburgh.\n",
      "We can apply our algorithm to this network much more quickly than the full set above.\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "assembler.assemble(\"forGAVIN/mergecode/edgelist_new.txt\",\n",
      "                   \"features/pulldown.edgelist.vectors.txt\", verbose=True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Reading pairfile: forGAVIN/mergecode/edgelist_new.txt\n",
        "Checking feature sizes:\n",
        "\t Data source forGAVIN/pulldown_data/dataset/ppi_ab_entrez.csv produces features of size 1.\n",
        "\t Data source affinityresults/results2/unique_data_ppi_coor_C2S_entrez.csv produces features of size 1.\n",
        "\t Data source Gene_Ontology produces features of size 90.\n",
        "\t Data source Y2H/Y2H.txt produces features of size 1.\n",
        "\t Data source ENTS produces features of size 107."
       ]
      }
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Test case\n",
      "\n",
      "We would like to test our model on well known interactions to verify that it can at least detect those reliably.\n",
      "The chosen test case to use are [RNA polymerases][rnapoly] as they are well-known.\n",
      "Searching the NCBI Gene database with keywords and looking for the different RNA polymerase IDs we can build the following lists:\n",
      "\n",
      "[rnapoly]: http://en.wikipedia.org/wiki/RNA_polymerase"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "f = open(\"polymerase/rnapolyresult.txt\")\n",
      "c = csv.reader(f, delimiter=\"\\t\")\n",
      "c.next()\n",
      "polydict = {}\n",
      "for l in c:\n",
      "    if l[5][:4] == 'POLR':\n",
      "        #then it's a polymerase\n",
      "        try:\n",
      "            polydict[l[5][4]] += [l[2]]\n",
      "        except KeyError:\n",
      "            polydict[l[5][4]] = [l[2]]\n",
      "f.close()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 23
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print polydict.keys()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "['1', '3', '2', 'M']\n"
       ]
      }
     ],
     "prompt_number": 24
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Getting rid of that M key:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "polydict.pop('M')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 25,
       "text": [
        "['5442']"
       ]
      }
     ],
     "prompt_number": 25
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Then we can just iterate over combinations within each of these complexes (all the interactions between genes in each complex should exist):"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import itertools"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 26
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "f = open(\"polymerase/interacting.pairs.rna.polymerase.txt\", \"w\")\n",
      "c = csv.writer(f,delimiter=\"\\t\")\n",
      "for k in polydict:\n",
      "    for pair in itertools.combinations(polydict[k],2):\n",
      "        c.writerow(pair)\n",
      "f.close()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 29
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Then we can generate the corresponding feature vectors:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "assembler.assemble(\"polymerase/interacting.pairs.rna.polymerase.txt\",\n",
      "                   \"features/interacting.pairs.rna.polymerase.vectors.txt\", verbose=True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Reading pairfile: polymerase/interacting.pairs.rna.polymerase.txt\n",
        "Checking feature sizes:\n",
        "\t Data source forGAVIN/pulldown_data/dataset/ppi_ab_entrez.csv produces features of size 1.\n",
        "\t Data source affinityresults/results2/unique_data_ppi_coor_C2S_entrez.csv produces features of size 1.\n",
        "\t Data source Gene_Ontology produces features of size 90.\n",
        "\t Data source Y2H/Y2H.txt produces features of size 1.\n",
        "\t Data source ENTS produces features of size 107.\n",
        "\t Data source iRefIndex produces features of size 11.\n",
        "\t Data source STRING produces features of size 8.\n",
        "\t Data source ENTS_summary produces features of size 1.\n",
        "\t Data source HMR produces features of size 1.\n",
        "\t Data source InterologWalk produces features of size 1.\n",
        "Writing feature vectors\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Wrote 310 vectors.\n",
        "Matched 100.00 % of protein pairs in polymerase/interacting.pairs.rna.polymerase.txt to features from forGAVIN/pulldown_data/dataset/ppi_ab_entrez.csv\n",
        "Matched 100.00 % of protein pairs in polymerase/interacting.pairs.rna.polymerase.txt to features from affinityresults/results2/unique_data_ppi_coor_C2S_entrez.csv\n",
        "Matched 100.00 % of protein pairs in polymerase/interacting.pairs.rna.polymerase.txt to features from Gene_Ontology\n",
        "Matched 100.00 % of protein pairs in polymerase/interacting.pairs.rna.polymerase.txt to features from Y2H/Y2H.txt\n",
        "Matched 24.52 % of protein pairs in polymerase/interacting.pairs.rna.polymerase.txt to features from ENTS\n",
        "Matched 100.00 % of protein pairs in polymerase/interacting.pairs.rna.polymerase.txt to features from iRefIndex\n",
        "Matched 100.00 % of protein pairs in polymerase/interacting.pairs.rna.polymerase.txt to features from STRING\n",
        "Matched 100.00 % of protein pairs in polymerase/interacting.pairs.rna.polymerase.txt to features from ENTS_summary\n",
        "Matched 100.00 % of protein pairs in polymerase/interacting.pairs.rna.polymerase.txt to features from HMR\n",
        "Matched 100.00 % of protein pairs in polymerase/interacting.pairs.rna.polymerase.txt to features from InterologWalk\n"
       ]
      }
     ],
     "prompt_number": 38
    }
   ],
   "metadata": {}
  }
 ]
}