{
 "metadata": {
  "name": "",
  "signature": "sha256:411ea4663d5416031258be5a4b45bd3dd8ff19e6a59746702cc27864c38966d4"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This notebook just contains rough notes on using parallel processing in an ipython notebook and with Scikit-learn.\n",
      "\n",
      "What processor do we have available on this server?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!lscpu"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Architecture:          x86_64\r\n",
        "CPU op-mode(s):        32-bit, 64-bit\r\n",
        "Byte Order:            Little Endian\r\n",
        "CPU(s):                24\r\n",
        "On-line CPU(s) list:   0-23\r\n",
        "Thread(s) per core:    2\r\n",
        "Core(s) per socket:    6\r\n",
        "Socket(s):             2\r\n",
        "NUMA node(s):          2\r\n",
        "Vendor ID:             GenuineIntel\r\n",
        "CPU family:            6\r\n",
        "Model:                 47\r\n",
        "Stepping:              2\r\n",
        "CPU MHz:               1862.106\r\n",
        "BogoMIPS:              3724.08\r\n",
        "Virtualisation:        VT-x\r\n",
        "L1d cache:             32K\r\n",
        "L1i cache:             32K\r\n",
        "L2 cache:              256K\r\n",
        "L3 cache:              18432K\r\n",
        "NUMA node0 CPU(s):     0,2,4,6,8,10,12,14,16,18,20,22\r\n",
        "NUMA node1 CPU(s):     1,3,5,7,9,11,13,15,17,19,21,23\r\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "For parallel processing with ipython there are [additional dependencies](http://ipython.org/ipython-doc/2/install/install.html#dependencies-for-ipython-parallel-parallel-computing).\n",
      "Looks like all we need is `pyzmq`:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!pip search pyzmq"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "pyzmq                     - Python bindings for 0MQ\r\n",
        "  INSTALLED: 14.3.1 (latest)\r\n",
        "gevent_zeromq             - gevent compatibility layer for pyzmq\r\n",
        "pyzmq-static              - Obsolete fork of pyzmq\r\n",
        "pyzmqrpc                  - A simple ZMQ RPC extension with JSON for message serialization\r\n",
        "pyzmq-ctypes              - Python bindings for 0MQ (ctypes version).\r\n",
        "pyzmq-wrapper             - Wrapper classes for pyzmq\r\n",
        "pseud                     - Bi-directional RPC API on top of pyzmq\r\n",
        "pyzmq-mdp                 - ZeroMQ MDP protocol in Python using pyzmq\r\n"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This [presentation][] may come in handy.\n",
      "\n",
      "# Ipython.parallel\n",
      "\n",
      "Looking at the documentation from [here][] on how to use Ipython's parallel tools.\n",
      "Not clear from the getting started where you're supposed to use the `ipcluster` command in an ipython notebook.\n",
      "Appears that in an ipython notebook it is as easy as going into the \"clusters\" tab on the home page.\n",
      "[This notebook][wakarinotes] explains how to start the cluster through ssh, which might be a better idea than what we're currently doing, but would probably take too long to set up at this point.\n",
      "\n",
      "[here]: http://ipython.org/ipython-doc/2/parallel/parallel_intro.html\n",
      "[presentation]: https://speakerdeck.com/datasciencelondon/parallel-and-large-scale-machine-learning-with-scikit-learn\n",
      "[wakarinotes]: https://www.wakari.io/sharing/bundle/ijstokes/ipcluster-wakari-intro"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "profile = \"default\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from IPython.parallel import *"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "client = Client(profile=profile)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "client.ids"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 5,
       "text": [
        "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print client[:].apply_sync(lambda : \"Hello, World\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "['Hello, World', 'Hello, World', 'Hello, World', 'Hello, World', 'Hello, World', 'Hello, World', 'Hello, World', 'Hello, World', 'Hello, World', 'Hello, World', 'Hello, World', 'Hello, World', 'Hello, World', 'Hello, World', 'Hello, World', 'Hello, World', 'Hello, World', 'Hello, World', 'Hello, World', 'Hello, World']\n"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Using these cores\n",
      "\n",
      "Looks like there's two ways to use these: Direct and Loadbalanced.\n",
      "\n",
      "### Direct\n",
      "\n",
      "Using these directly is described on [this page][ipyd].\n",
      "\n",
      "> The basic idea behind the multiengine interface is that the capabilities of each engine are directly and explicitly exposed to the user. Thus, in the multiengine interface, each engine is given an id that is used to identify the engine and give it work to do. This interface is very intuitive and is designed with interactive usage in mind, and is the best place for new users of IPython to begin.\n",
      "\n",
      "So it's simpler and easier to work with, sounds good.\n",
      "\n",
      "[ipyd]: http://ipython.org/ipython-doc/2/parallel/parallel_multiengine.html#parallel-multiengine"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "dview = client[:]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%timeit\n",
      "serial_result = map(lambda x:x**10, range(1000000))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "1 loops, best of 3: 2.41 s per loop\n"
       ]
      }
     ],
     "prompt_number": 17
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%timeit\n",
      "parallel_result = dview.map_sync(lambda x: x**10, range(1000000))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "1 loops, best of 3: 652 ms per loop\n"
       ]
      }
     ],
     "prompt_number": 18
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Would have thought that would speed up a bit more.\n",
      "\n",
      "Trying it out asynchronously:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "parallel_result = dview.map_async(lambda x: x**10, range(1000000))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 19
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now the `parallel_result` will be an `AsyncResult` object:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print parallel_result"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "<AsyncMapResult: <lambda>>\n"
       ]
      }
     ],
     "prompt_number": 20
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can get the actual result, if it's ready, using the `.get` method:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print type(parallel_result.get())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "<type 'list'>\n"
       ]
      }
     ],
     "prompt_number": 22
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Could probably hack together most of what I'll want to do with just that `dview.map_async` method."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Ipython parallel magics\n",
      "\n",
      "Doing the above with [the ipython parallel magics][magics].\n",
      "\n",
      "Don't really understand the point of executing the same thing on many cores.\n",
      "What I want to do is execute different things, surely?\n",
      "\n",
      "[magics]: http://ipython.org/ipython-doc/2/parallel/magics.html#parallel-magics"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%px --noblock\n",
      "x = \"something\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 25,
       "text": [
        "<AsyncResult: finished>"
       ]
      }
     ],
     "prompt_number": 25
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%pxresult"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 26
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%px\n",
      "print x"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[stdout:0] something\n",
        "[stdout:1] something\n",
        "[stdout:2] something\n",
        "[stdout:3] something\n",
        "[stdout:4] something\n",
        "[stdout:5] something\n",
        "[stdout:6] something\n",
        "[stdout:7] something\n",
        "[stdout:8] something\n",
        "[stdout:9] something\n",
        "[stdout:10] something\n",
        "[stdout:11] something\n",
        "[stdout:12] something\n",
        "[stdout:13] something\n",
        "[stdout:14] something\n",
        "[stdout:15] something\n",
        "[stdout:16] something\n",
        "[stdout:17] something\n",
        "[stdout:18] something\n",
        "[stdout:19] something\n"
       ]
      }
     ],
     "prompt_number": 28
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Would be useful to be able to execute a non-blocking job on a single core and be able to just leave it running.\n",
      "Then later, come back to it and get the results."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%pxconfig --noblock --targets 0"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 29
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%px print \"something\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 30,
       "text": [
        "<AsyncResult: execute>"
       ]
      }
     ],
     "prompt_number": 30
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%pxresult"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "something\n"
       ]
      }
     ],
     "prompt_number": 31
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%autopx"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "%autopx enabled\n"
       ]
      }
     ],
     "prompt_number": 45
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "a = \"Can execute arbitrary code on this core.\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 46
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%autopx"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "%autopx disabled\n"
       ]
      }
     ],
     "prompt_number": 47
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "a = dview.pull('a',targets=0)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 53
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "a.get()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 54,
       "text": [
        "'Can execute arbitrary code on this core.'"
       ]
      }
     ],
     "prompt_number": 54
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Actually, executing the same thing on different cores might not be such a bad idea.\n",
      "I could send different folds of the data to different cores and then execute the same fitting and test commands on all of them.\n",
      "Then, I could just pull in the results from all the cores."
     ]
    }
   ],
   "metadata": {}
  }
 ]
}