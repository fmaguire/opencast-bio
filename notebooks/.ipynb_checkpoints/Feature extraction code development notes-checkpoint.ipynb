{
 "metadata": {
  "name": "",
  "signature": "sha256:c675284186ab44b81a98ecaff252c067e4770a30b15c2c720e7d82c013b9d906"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Iteration 2\n",
      "\n",
      "Due to the problem of extremely large database files being produced when extracting features with extremely good coverage, such as Gene Ontology, a new version of the code is required to deal with this problem.\n",
      "The new code will store the ProteinPairParser objects as pickle files and the features will be indexed from these objects through a `__getitem__` method with the ProteinPairParser only interacting with it's database, if it has one.\n",
      "\n",
      "Each ProteinPairParser will have it's own generator function which will either be created using the options handed to it or loaded from another pickle file.\n",
      "The default generator will act as the code currently does, by creating a database then indexing said database to retrieve files.\n",
      "According to the Python documentation if the Parser opens the database at initialisation and then is pickled the database will be closed and opened again at unpickling time: [pickle documentation][pickledocs] - see the TextReader example.\n",
      "\n",
      "\n",
      "\n",
      "[pickledocs]: https://docs.python.org/2/library/pickle.html#example"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "cd /home/gavin/Documents/MRes/"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "/home/gavin/Documents/MRes\n"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Going to step through the changes I'm making to the code and list them using git.\n",
      "After making changes I'll run a test case to make sure it's still working."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import sys\n",
      "sys.path.append(\"/home/gavin/Documents/MRes/opencast-bio/\")\n",
      "import ocbio.extract"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "reload(ocbio.extract)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 11,
       "text": [
        "<module 'ocbio.extract' from '/home/gavin/Documents/MRes/opencast-bio/ocbio/extract.py'>"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Changing parsers to use __getitem__\n",
      "\n",
      "This involves ensuring that at initialisation the parser will define a function to return values by default from the database it's going to create when `regenerate` is run.\n",
      "There must also be an option to load an arbitrary pickled object to return items.\n",
      "\n",
      "Also, the databases must now be opened and closed with the parsers and the opened databases stored within the parser objects.\n",
      "The assembler will have to be modified to deal with this and close the databases when it's done.\n",
      "\n",
      "Showing these changes:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!git show HEAD"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\u001b[33mcommit 9aa9c8ae4e00146ca0b3fc6ac510fbcf67034e77\u001b[m\r\n",
        "Author: Gavin <gavingray1729@gmail.com>\r\n",
        "Date:   Sun Jun 22 10:37:09 2014 +0100\r\n",
        "\r\n",
        "    debugged, works now\r\n",
        "\r\n",
        "\u001b[1mdiff --git a/ocbio/extract.py b/ocbio/extract.py\u001b[m\r\n",
        "\u001b[1mindex b1c37f6..0bb7db8 100644\u001b[m\r\n",
        "\u001b[1m--- a/ocbio/extract.py\u001b[m\r\n",
        "\u001b[1m+++ b/ocbio/extract.py\u001b[m\r\n",
        "\u001b[36m@@ -8,6 +8,19 @@\u001b[m \u001b[mimport shelve\u001b[m\r\n",
        " import re\u001b[m\r\n",
        " import sys\u001b[m\r\n",
        " \u001b[m\r\n",
        "\u001b[32m+\u001b[m\u001b[32mdef verbosecheck(verbose):\u001b[m\r\n",
        "\u001b[32m+\u001b[m\u001b[32m    '''returns a function depending on the state of the verbose flag'''\u001b[m\r\n",
        "\u001b[32m+\u001b[m\u001b[32m    if verbose:\u001b[m\r\n",
        "\u001b[32m+\u001b[m\u001b[32m        def v_print(*args):\u001b[m\r\n",
        "\u001b[32m+\u001b[m\u001b[32m            '''declare v_print function that prints to stdout\u001b[m\r\n",
        "\u001b[32m+\u001b[m\u001b[32m            if verbose flag is on'''\u001b[m\r\n",
        "\u001b[32m+\u001b[m\u001b[32m            for argument in args:\u001b[m\r\n",
        "\u001b[32m+\u001b[m\u001b[32m                print argument,\u001b[m\r\n",
        "\u001b[32m+\u001b[m\u001b[32m                print\u001b[m\r\n",
        "\u001b[32m+\u001b[m\u001b[32m    else:\u001b[m\r\n",
        "\u001b[32m+\u001b[m\u001b[32m        def v_print(*args):\u001b[m\r\n",
        "\u001b[32m+\u001b[m\u001b[32m            None\u001b[m\r\n",
        "\u001b[32m+\u001b[m\u001b[32m    return v_print\u001b[m\r\n",
        " \u001b[m\r\n",
        " class FeatureVectorAssembler():\u001b[m\r\n",
        "     '''Assembles feature vectors from protein pair files, data source lists\u001b[m\r\n",
        "\u001b[36m@@ -18,16 +31,7 @@\u001b[m \u001b[mclass FeatureVectorAssembler():\u001b[m\r\n",
        "         # store the directory of the table and it's name\u001b[m\r\n",
        "         self.sourcetabdir, self.tabfile = os.path.split(sourcetab)\u001b[m\r\n",
        " \u001b[m\r\n",
        "\u001b[31m-        if verbose:\u001b[m\r\n",
        "\u001b[31m-            def v_print(*args):\u001b[m\r\n",
        "\u001b[31m-                '''declare v_print function that prints to stdout\u001b[m\r\n",
        "\u001b[31m-                if verbose flag is on'''\u001b[m\r\n",
        "\u001b[31m-                for argument in args:\u001b[m\r\n",
        "\u001b[31m-                    print argument,\u001b[m\r\n",
        "\u001b[31m-                    print\u001b[m\r\n",
        "\u001b[31m-        else:\u001b[m\r\n",
        "\u001b[31m-            def v_print(*args):\u001b[m\r\n",
        "\u001b[31m-                None\u001b[m\r\n",
        "\u001b[32m+\u001b[m\u001b[32m        v_print = verbosecheck(verbose)\u001b[m\r\n",
        " \u001b[m\r\n",
        "         v_print(\"Using {0} from top data directory {1}.\".format(self.sourcetabdir,\u001b[m\r\n",
        "                                                                 self.tabfile))\u001b[m\r\n",
        "\u001b[36m@@ -83,6 +87,7 @@\u001b[m \u001b[mclass FeatureVectorAssembler():\u001b[m\r\n",
        "     def regenerate(self, force=False, verbose=False):\u001b[m\r\n",
        "         '''Calls all known protein parsers and gets them to regenerate their\u001b[m\r\n",
        "         output, if they have to.'''\u001b[m\r\n",
        "\u001b[32m+\u001b[m\u001b[32m        v_print = verbosecheck(verbose)\u001b[m\r\n",
        "         v_print(\"Regenerating parsers:\")\u001b[m\r\n",
        "         for parser in self.parserlist:\u001b[m\r\n",
        "             v_print(\"\\t parser {0}\".format(self.parserlist.index(parser)))\u001b[m\r\n",
        "\u001b[36m@@ -94,6 +99,7 @@\u001b[m \u001b[mclass FeatureVectorAssembler():\u001b[m\r\n",
        "         '''Assembles a file of feature vectors for each protein pair in a\u001b[m\r\n",
        "         protein pair file supplied.\u001b[m\r\n",
        "         Assumes the pairfile is tab delimited.'''\u001b[m\r\n",
        "\u001b[32m+\u001b[m\u001b[32m        v_print = verbosecheck(verbose)\u001b[m\r\n",
        "         v_print(\"Reading pairfile: {0}\".format(pairfile))\u001b[m\r\n",
        "         # first parse the pairfile into a list of frozensets\u001b[m\r\n",
        "         pairs = map(lambda l: frozenset(l), csv.reader(open(pairfile), delimiter=\"\\t\"))\u001b[m\r\n",
        "\u001b[36m@@ -224,6 +230,7 @@\u001b[m \u001b[mclass ProteinPairParser():\u001b[m\r\n",
        "     def regenerate(self, force=False, verbose=False):\u001b[m\r\n",
        "         '''Regenerate the pair file from the data source\u001b[m\r\n",
        "         if the data source is newer than the pair file'''\u001b[m\r\n",
        "\u001b[32m+\u001b[m\u001b[32m        v_print = verbosecheck(verbose)\u001b[m\r\n",
        "         # so first check the ages of both files\u001b[m\r\n",
        "         datamtime = os.stat(self.datadir)[-2]\u001b[m\r\n",
        "         if os.path.isfile(self.outdir):\u001b[m\r\n"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Testing initialisation:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "testparser = ocbio.extract.ProteinPairParser(\"none\",\"none\",generator=\"geneontology/retreival.function.pickle\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 16
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "f = open(\"GOfeatures.class.test\")\n",
      "GOfeatures1 = pickle.load()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "GOfeatures = None"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 17
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "ocbio.extract.ProteinPairParser?"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 12
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Iteration 1\n",
      "\n",
      "These are the notes on the development of the code described in the [Feature vector assembly][featureassembly] page of the wiki.\n",
      "First, the protein pair parser class will be written and tested on a dataset that has already been extracted.\n",
      "This will be the HIPPIE dataset.\n",
      "\n",
      "Initialisation currently involves loading in the three command line options and saving them to the object.\n",
      "It must also involve parsing of the options.\n",
      "Testing the initialisation:\n",
      "\n",
      "[featureassembly]: https://github.com/ggray1729/opencast-bio/wiki/Feature-vector-assembly"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import os, time, subprocess, csv, shelve"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 27
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "cd /home/gavin/Documents/MRes/HIPPIE/"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "/home/gavin/Documents/MRes/HIPPIE\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#define the class\n",
      "class ProteinPairParser():\n",
      "    '''Does simple parsing on data files to produce protein pair files with feature values'''\n",
      "    def __init__(self,datadir,outdir,protindexes=(1,2),valindex=3,script=None,csvdelim=\"\\t\"):\n",
      "        #first, store the initialisation\n",
      "        self.datadir = datadir\n",
      "        self.outdir = outdir\n",
      "        self.protindexes=protindexes\n",
      "        self.valindex=valindex\n",
      "        self.script=script\n",
      "        self.csvdelim=csvdelim\n",
      "        return None\n",
      "    \n",
      "    def regenerate(self):\n",
      "        '''Regenerate the pair file from the data source\n",
      "        if the data source is newer than the pair file'''\n",
      "        # so first check the ages of both files\n",
      "        datamtime = os.stat(self.datadir)[-2]\n",
      "        if os.path.isfile(self.outdir):\n",
      "            pairmtime = os.stat(self.outdir)[-2]\n",
      "        else:\n",
      "            #bit of a hack\n",
      "            pairmtime = 0\n",
      "        #if the data modification time is greater than output modification time\n",
      "        if datamtime > pairmtime:\n",
      "            # now regenerate the data file according to the options defined above:\n",
      "            print \"data file is newer than pair file\"\n",
      "            #if there's a script to run\n",
      "            if self.script != None:\n",
      "                #then execute the script\n",
      "                retcode=subprocess.call(\"python2 %s\"%self.script, shell=True)\n",
      "            #first delete out of date file, if it's there\n",
      "            if os.path.isfile(self.outdir):\n",
      "                os.remove(self.outdir)\n",
      "            #perform simple parsing to make a file of just protein pairs and the value we care about\n",
      "            #and save these using shelve\n",
      "            db = openpairshelf(self.outdir)\n",
      "            #open the data file\n",
      "            c = csv.reader(open(self.datadir), delimiter=self.csvdelim)\n",
      "            for line in c:\n",
      "                #each line use the protein pair as a key\n",
      "                #by formatting it as a frozenset\n",
      "                pair = frozenset([line[self.protindexes[0]],line[self.protindexes[1]]])\n",
      "                #and the value is indexed by valindex\n",
      "                db[pair] = line[self.valindex]\n",
      "            db.close()\n",
      "        return None"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 95
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We have to rerun the script to generate a pre-processed HIPPIE file:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%bash\n",
      "java -jar HIPPIE_NC.jar -i=../DIP/human/training.nolabel.positive.Entrez.txt -t=e -l=0 -o=prematch.positive.HIPPIE.txt"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 82
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Then we can use the parser to quickly perform what is done in the notebook previously used to extract the features:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "x = ProteinPairParser(\"prematch.positive.HIPPIE.txt\",\"training.positive.HIPPIE.txt\",protindexes=(1,3),valindex=4)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 113
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "x.regenerate()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "data file is newer than pair file\n"
       ]
      }
     ],
     "prompt_number": 114
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The function reports that the data file is newer than the pair file and regenerates the pair files as a persistent dictionary object.\n",
      "This is useful because it means that this can later be indexed quickly for building feature vectors.\n",
      "\n",
      "We can open this database to show this functionality:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "test = openpairshelf(\"training.positive.HIPPIE.txt\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 115
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "If we look at the keys that this database uses we can see that it is using strings internally.\n",
      "It could be useful to modify the `.keys()` method of this function so that this would produce the list of frozenset keys instead."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "test.keys()[0:10]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 116,
       "text": [
        "['10013\\t55031',\n",
        " '10094\\t2885',\n",
        " '10399\\t8364',\n",
        " '10971\\t8452',\n",
        " '1109111091',\n",
        " '11198\\t142',\n",
        " '1326\\t9020',\n",
        " '13917118\\t7322',\n",
        " '1869\\t142',\n",
        " '2\\t2885']"
       ]
      }
     ],
     "prompt_number": 116
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "And taking one of these keys and turning it into a frozenset, we can then index the database using that as a key."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "testkey = test.keys()[0]\n",
      "testkey = frozenset(testkey.split(\"\\t\"))\n",
      "print testkey\n",
      "test[testkey]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "frozenset(['10013', '55031'])\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 117,
       "text": [
        "'0.72'"
       ]
      }
     ],
     "prompt_number": 117
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "test.close()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 118
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The problem is that a shelve (shelf?) database can't take the frozenset as a key.\n",
      "The [recommended way to deal with this][stackshelve] is to make a wrapper.\n",
      "As the database used by shelf is a class we can build a child class from this, modifying the functions to deal with protein pairs stored in frozensets as keys.\n",
      "This code will not deal with arbitrary frozensets as keys.\n",
      "\n",
      "Essentially, it will use a string of the two protein identifiers separated by a tab as the key.\n",
      "To index though it will take a frozenset and convert it to two strings which are the two iterations of the two strings.\n",
      "\n",
      "[stackshelve]: http://stackoverflow.com/questions/19734821/why-does-pythons-shelve-require-that-all-keys-be-strings"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class ProteinPairDB(shelve.DbfilenameShelf):\n",
      "    '''A simple database for protein pairs using shelve.'''\n",
      "    def __setitem__(self,key,value):\n",
      "        #key will be frozenset so make it a list first\n",
      "        key = list(key)\n",
      "        #then make it a string\n",
      "        if len(key) == 1:\n",
      "            key = key[0]*2\n",
      "        else:\n",
      "            key = key[0] + \"\\t\" + key[1]\n",
      "        shelve.DbfilenameShelf.__setitem__(self,key,value)\n",
      "        return None\n",
      "    \n",
      "    def __getitem__(self,key):\n",
      "        #make two strings from the key\n",
      "        key = list(key)\n",
      "        if len(key) == 1:\n",
      "            key1 = key[0]*2\n",
      "            key1 = key[0]*2\n",
      "        else:\n",
      "            key1 = key[0] + \"\\t\" + key[1]\n",
      "            key2 = key[1] + \"\\t\" + key[0]\n",
      "        #try the first one\n",
      "        try:\n",
      "            value = shelve.DbfilenameShelf.__getitem__(self,key1)\n",
      "        except KeyError:\n",
      "            #couldn't find the first key, try the second\n",
      "            value = shelve.DbfilenameShelf.__getitem__(self,key2)\n",
      "            #if we don't find this one then error out as usual\n",
      "        return value"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 110
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We also have to define a function to apply default arguments to have similar functionality to shelve:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def openpairshelf(filename, flag='c', protocol=None, writeback=False):\n",
      "    return ProteinPairDB(filename, flag, protocol, writeback)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 57
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The next thing to do is write the code for the feature vector assembler.\n",
      "This is another class, with methods:\n",
      "\n",
      "1. Regenerate protein pairs:\n",
      "    * Sends a signal to all instantiated protein pair parsers to regenerate the pair file if required (if the data source has been updated).\n",
      "2. Initialise protein pairs from data source table.\n",
      "3. Assemble feature vector files from protein pair files.\n",
      "\n",
      "And at initialisation it is expected to do:\n",
      "\n",
      "1. Loading in the Data source table and instantiating all required protein pair parsers.\n",
      "2. Loading in the gold standard protein pairs files, or loading the file names.\n",
      "\n",
      "To do this it must parse the data source table.\n",
      "It assumes that the data source table is provided as a __full path__ and that this path is __the top directory for the data__.\n",
      "ie all of the data paths in the data source will be __relative to the path of the table itself__.\n",
      "The table itself will have structure:\n",
      "\n",
      "| Data source directory | Output database directory | Options |\n",
      "| --------- | --------- | ------- |\n",
      "| `/relative/path/to/data` | `/relative/path/to/output/database` | protindexes=1,3;valindex=4;script=`/path/to/script`;csvdelim=`\\t` |\n",
      "\n",
      "The available options are given in the options column of the table above.\n",
      "\n",
      "To test initialisation a first draft of the code is given below:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class FeatureVectorAssembler():\n",
      "    '''Assembles feature vectors from protein pair files, data source lists and gold standard protein pair lists.'''\n",
      "    def __init__(self,sourcetab,goldpos,goldneg):\n",
      "        #store the initialisation\n",
      "        # directories for positive and negative gold standard data files\n",
      "        self.goldpos = goldpos\n",
      "        self.goldneg = goldneg\n",
      "        \n",
      "        #check if the gold standard data directories exist\n",
      "        # throw an error if they don't\n",
      "        \n",
      "        #instantiate protein pair parsers\n",
      "        # first parse the data source table\n",
      "        # store the directory of the table and it's name\n",
      "        self.sourcetabdir,self.tabfile = os.path.split(sourcetab)\n",
      "        # open the table and parse for initialisation options\n",
      "        c = csv.reader(open(sourcetab), delimiter=\"\\t\")\n",
      "        # iterate over lines adding to list of protein pair parsers\n",
      "        self.parserinitlist = []\n",
      "        for line in c:\n",
      "            #store the information in a dictionary\n",
      "            d = {}\n",
      "            d[\"data path\"] = line[0]\n",
      "            d[\"output path\"] = line[1]\n",
      "            #store options in a dictionary in the dictionary\n",
      "            d[\"options\"] = {}\n",
      "            options = line[2].split(\";\")\n",
      "            for x in options:\n",
      "                #split each option to find out which option it is:\n",
      "                x = x.split(\"=\")\n",
      "                #store it in the dictionary\n",
      "                # if there are invalid options this code WILL NOT DETECT THEM\n",
      "                d[\"options\"][x[0]]= x[1]\n",
      "            #copy the dictionary into the list\n",
      "            self.parserinitlist.append(d.copy())\n",
      "        #then initialise each of these parsers and keep them in a list\n",
      "        self.parserlist = []\n",
      "        for parser in self.parserinitlist:\n",
      "            self.parserlist.append(ProteinPairParser(parser[\"data path\"],\n",
      "                                                     parser[\"output path\"],\n",
      "                                                     **parser[\"options\"]))\n",
      "        return None"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 147
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Testing initialisation requires a data source table file so this file is created below at the top directory for the data files."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "cd /home/gavin/Documents/MRes/"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "/home/gavin/Documents/MRes\n"
       ]
      }
     ],
     "prompt_number": 139
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "f = open(\"datasource.tab\", \"w\")\n",
      "f.write(\"HIPPIE/prematch.positive.HIPPIE.txt\" + \"\\t\" + \"HIPPIE/training.positive.HIPPIE.txt\" + \"\\t\" + \"protindexes=(1,3);valindex=4\")\n",
      "f.close()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 141
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Checking this file has written properly:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%bash\n",
      "cat datasource.tab"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "HIPPIE/prematch.positive.HIPPIE.txt\tHIPPIE/training.positive.HIPPIE.txt\tprotindexes=(1,3);valindex=4"
       ]
      }
     ],
     "prompt_number": 143
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Then attempt to initialise the above version from that.\n",
      "Notice here that the FeatureVectorAssembler requires gold standard data sources at initialisation in this version.\n",
      "This is removed in the second version as it made more sense to allow arbitrary protein pair lists in the feature vector assemble method."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "test = FeatureVectorAssembler(\"datasource.tab\",\n",
      "                              \"DIP/human/training.nolabel.positive.Entrez.txt\",\n",
      "                              \"DIP/human/training.nolabel.negative.Entrez.txt\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 148
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "It initialises ok, so the second draft of the code with the required methods is given below:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class FeatureVectorAssembler():\n",
      "    '''Assembles feature vectors from protein pair files, data source lists and gold standard protein pair lists.'''\n",
      "    def __init__(self,sourcetab):\n",
      "        #instantiate protein pair parsers\n",
      "        # first parse the data source table\n",
      "        # store the directory of the table and it's name\n",
      "        self.sourcetabdir,self.tabfile = os.path.split(sourcetab)\n",
      "        # open the table and parse for initialisation options\n",
      "        c = csv.reader(open(sourcetab), delimiter=\"\\t\")\n",
      "        # iterate over lines adding to list of protein pair parsers\n",
      "        self.parserinitlist = []\n",
      "        for line in c:\n",
      "            #store the information in a dictionary\n",
      "            d = {}\n",
      "            d[\"data path\"] = os.path.join(self.sourcetabdir,line[0])\n",
      "            d[\"output path\"] = os.path.join(self.sourcetabdir,line[1])\n",
      "            #store options in a dictionary in the dictionary\n",
      "            d[\"options\"] = {}\n",
      "            options = line[2].split(\";\")\n",
      "            for x in options:\n",
      "                #split each option to find out which option it is:\n",
      "                x = x.split(\"=\")\n",
      "                #store it in the dictionary\n",
      "                # if there are invalid options this code WILL NOT DETECT THEM\n",
      "                d[\"options\"][x[0]]= x[1]\n",
      "            #update the script directory\n",
      "            if \"script\" in d[\"options\"].keys():\n",
      "                d[\"options\"][\"script\"] = os.path.join(self.sourcetabdir,d[\"options\"][\"script\"])\n",
      "            #copy the dictionary into the list\n",
      "            self.parserinitlist.append(d.copy())\n",
      "        #then initialise each of these parsers and keep them in a list\n",
      "        self.parserlist = []\n",
      "        for parser in self.parserinitlist:\n",
      "            self.parserlist.append(ProteinPairParser(parser[\"data path\"],\n",
      "                                                     parser[\"output path\"],\n",
      "                                                     **parser[\"options\"]))\n",
      "        return None\n",
      "    \n",
      "    def regenerate(self):\n",
      "        '''Calls all known protein parsers and gets them to regenerate their output, if they have to.'''\n",
      "        for parser in self.parserlist:\n",
      "            parser.regenerate()\n",
      "        return None\n",
      "    \n",
      "    def assemble(self, pairfile, outputfile):\n",
      "        '''Assembles a file of feature vectors for each protein pair in a protein pair file supplied.\n",
      "        \n",
      "        Assumes the pairfile is tab delimited.'''\n",
      "        # first parse the pairfile into a list of frozensets\n",
      "        pairs = map(lambda l: frozenset(l),csv.reader(open(pairfile), delimiter=\"\\t\"))\n",
      "        # open the file to put the feature vector in\n",
      "        c = csv.writer(open(outputfile, \"w\"), delimiter=\"\\t\")\n",
      "        #open all the databases and put them in a dictionary\n",
      "        dbdict = {}\n",
      "        for parser in self.parserinitlist:\n",
      "            dbdict[parser[\"output path\"]] = openpairshelf(parser[\"output path\"])\n",
      "        \n",
      "        # then iterate through the pairs, querying all parser databases\n",
      "        for pair in pairs:\n",
      "            row = []\n",
      "            lpair = list(pair)\n",
      "            row = row + lpair\n",
      "            for parser in self.parserinitlist:\n",
      "                row.append(dbdict[parser[\"output path\"]][pair])\n",
      "            c.writerow(row)\n",
      "            \n",
      "        #close all the databases\n",
      "        for parser in self.parserinitlist:\n",
      "            dbdict[parser[\"output path\"]].close()\n",
      "        \n",
      "        return None"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 174
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Testing initialisation of this code again:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "test = FeatureVectorAssembler(\"/home/gavin/Documents/MRes/datasource.tab\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 175
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Regenerating the data sources.\n",
      "It does not report that any of the data sources require regeneration so nothing is done this time:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "test.regenerate()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 176
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Then testing the assembly of a feature vector file:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "test.assemble(\"DIP/human/training.nolabel.positive.Entrez.txt\", \"testoutput\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 177
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Looking at this file we can see that it has produced a feature vector with only one feature.\n",
      "It also reports the associated protein pairs next to this feature.\n",
      "This is removed from later versions as it makes later classification more complicated."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%bash\n",
      "head testoutput"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "4084\t207\t0.86\r\n",
        "8360\t8356\t0.97\r\n",
        "5914\t9612\t0.9\r\n",
        "79833\t6634\t0.62\r\n",
        "29102\t4090\t0\r\n",
        "7074\t6382\t0\r\n",
        "7159\t22059\t0\r\n",
        "1869\t7029\t0.96\r\n",
        "801\t817\t0\r\n",
        "207\t1786\t0.7\r\n"
       ]
      }
     ],
     "prompt_number": 178
    }
   ],
   "metadata": {}
  }
 ]
}