\chapter{Notebooks}
\label{app:notebooks}

The job of quickly running arbitrary processing on a variety of different data sources, each of which are being encountered for the first time was approached using IPython notebooks.
Quick interactive programming was useful as unexpected problems could be quickly solved.
Also, a detailed log, with inline comments, could be kept to track exactly what was done.

It would be possible for anyone with access to the data to run this code again to verify it.
%FIN: as has been seen in some academic publications recently http://ged.msu.edu/papers/2012-diginorm/
The code was run with Python version 2.7.7 and Scikit-learn 0.15.0.
The notebooks can be found at the following locations:

%link to root directory
\begin{itemize}
    \item The root directory for these notebooks can be found here: \url{https://github.com/ggray1729/opencast-bio/tree/master/notebooks}
    \item These can be viewed here: \url{http://nbviewer.ipython.org/github/ggray1729/opencast-bio/tree/master/notebooks/}
\end{itemize}

\section{Feature Extraction Notebooks}

Of the feature extraction notebooks in the repository, not all of them were successful.
Only those that were successful are listed here.

\subsection{Gene Ontology}
\label{app:gonotes}

In total 90 features were extracted from the Gene Ontology as binary values.
The following notebook describes how the features applied were generated:

\begin{itemize}
    \item The notebook in the opencast-bio repository can be found here: \url{https://github.com/ggray1729/opencast-bio/blob/master/notebooks/Extracting%20Gene%20Ontology%20features%200.2.ipynb}
        \item This notebook can be viewed here: \url{http://nbviewer.ipython.org/github/ggray1729/opencast-bio/blob/master/notebooks/Extracting%20Gene%20Ontology%20features%200.2.ipynb}
\end{itemize}

\subsection{Features derived from ENTS}

%pending
107 features were generated derived from the work in \textcite{rodgers-melnick_predicting_2013}.
These were found by analysing the code provided on the papers web page as described in the following notebook:

\begin{itemize}
    \item The notebook in the opencast-bio repository can be found here: \url{https://github.com/ggray1729/opencast-bio/blob/master/notebooks/Inspecting%20ENTS%20code.ipynb}
        \item This notebook can be viewed here: \url{http://nbviewer.ipython.org/github/ggray1729/opencast-bio/blob/master/notebooks/Inspecting%20ENTS%20code.ipynb}
\end{itemize}

\section{Classifier Training}
\label{app:classtrain}

This notebook contains all the code that was run to train and test the classifier used in this project.
It involves model selection, grid search of parameters and various plots describing the performance of different classifiers, such as \ac{ROC} curves and Precision Recall curves.
Links are provided to the code and an online service to view the notebook:

\begin{itemize}
    \item The notebook in the opencast-bio repository can be found here: \url{https://github.com/ggray1729/opencast-bio/blob/master/notebooks/Classifier%20Training.ipynb}
        \item This notebook can be viewed here: \url{http://nbviewer.ipython.org/github/ggray1729/opencast-bio/blob/master/notebooks/Classifier%20Training.ipynb}
\end{itemize}

In this notebook both learning curves and pipelines are used in the training of the classifiers.
These are described in the following sections:

\subsubsection*{Pipeline}

%what is a pipeline? why did I use one?
A pipeline is a combination of algorithms intended to run on the data in sequence after the data has been split into training and test.
In the case of this project the pipeline involved three components: a mean value filling imputer, a standard scaler and the classifier itself.
The imputer simply replaced missing values in the training data with the corresponding mean value for that column.
Scikit-learn's standard scaler centers the data at zero mean and unit variance.

%FIN: need to explain/just why you need to normalise data and gracefully handle missing data

\subsubsection*{Learning Curves}
% learning curves, what are they? and why?
Learning curves, as in the notebook referenced in Appendix \ref{app:classtrain}, were used in this project to ensure that the number of samples used in a grid search was sufficient.
The learning curve plots the accuracy of a classifier after it has been trained using cross-validation on a varying number of samples.
%FIN: most importantly they can diagnose common classifier issues (bias/variance) 
%example learning curve?
%still pending, worth it if page count allows



\section{ocbio.extract Usage}
\label{app:ocbio}

This notebook describes how to use the code developed in the project to build feature vectors from the various data sources.
It can be found in the following locations:

\begin{itemize}
    \item The notebook in the opencast-bio repository can be found here: \url{https://github.com/ggray1729/opencast-bio/blob/master/notebooks/ocbio.extract%20usage%20notes.ipynb}
    \item This notebook can be viewed here: \url{http://nbviewer.ipython.org/github/ggray1729/opencast-bio/blob/master/notebooks/ocbio.extract%20usage%20notes.ipynb}
\end{itemize}

