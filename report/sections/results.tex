\chapter{Results}
\label{results}

%intro to the results

%iterations of results - DIP, HIPPIE, Bayesian updating and reasoning behind it
% can then refer to this throughout this section.
As the project progressed the intended approach was found to be flawed and some changes were made.
The first of these was the change from a DIP-based training set to HIPPIE-based training set.
After the classification had been performed the use of a supervised classifier with this training set was found to be a poor method by itself for weighting interactions.
Using all of the data available it was possible to run a simple Bayesian alternative to continue with the weighting for the Community detection algorithm.

%Feature extraction results
\section{PPI feature vectors}

%The features extracted were X,Y,Z and the appendices explaining how this was done are A,B,C.
Many features were identified for extraction and these are described in Appendix \ref{datasources}.
Of these, only a small subset were succesfully processed into a usable form.
These were:

\begin{itemize}
    \item
\end{itemize}

Of these, a smaller proportion were used in the final classifier, which neglected features directly derived from interaction databases.
The features used to train the final classifier are shown in table \ref{tab:features}.
A brief description of these features is given below.

%feature table, with information about each
\begin{table}
    \centering
    %tabular goes here
    \caption{A table summarising the components of the feature vectors used in the final classifier.}
    \label{tab:features}
\end{table}

\subsection{Gene Ontology}

%Larger features
%Gene ontology was built as a feature in the same manner as that of qi_evaluation_2006 but, without knowing their approach, we had to develop our own method of creating usable features.
The Gene Ontology\autocite{ashburner_gene_2000} is a resource of annotations for genes to indicate various characteristics in a hierarchical manner, such as cellular localisation or function.
This resource has been used in past papers\autocite{qi_evaluation_2006} and in databases such as STRING\autocite{von-mering_string:_2005} to predict protein interactions.
Intuitively, it can be used to detect when, for example, two proteins are localised in the same area of the cell - as this would increase the probability that these two proteins interact.
Details on exactly how this feature was generated can be found in the notebook reference in Appendix \ref{app:go}.

\subsection{Features derived from ENTS}
%ENTS features were retreived through analysis and modification of the code published on the ENTS website, but did not have full coverage on any dataset.

These features were obtained through analysis and modification of the bundled code and data downloaded from the website of \textcite{rodgers-melnick_predicting_2013}.
In turn, most of these features were generated through the Multiloc2 program of \textcite{blum_multiloc2:_2009}.
The remaining features are pairwise combinations of conserved protein domains, which are conserved "modules" of proteins described in \textcite{janin_domains_1985}.

\subsection{Yeast Two-Hybrid}

%description of Y2H feature

\subsection{Removed features}

%Before their removal the features that best predicted the chosen gold standard dataset were reliably those directly derived from interaction databases.

%example feature importance graph

%After removing interaction databases 

%graph of feature importance from RF classifier?

%tables with explanation - table above now, not required.

\subsection{Data visualisation}

%describe the problem of using in proportion and out of proportion methods

\subsubsection{Reducing dimensionality}
%PCA is a relatively simply and fast way to reduce the high dimensionality of our feature vector into a form that can be easily plotted.

%tSNE is more complicated, but was recommended due to reportedly good performance

%Both methods show that this data is likely to be difficult to accurately categorize as the points are not separated in a 2d space

\subsection{High dimensional plots}

%Very few graphs are able to integrate large numbers of dimensions in a meaningful way; parallel line graphs and Andrew's curves are the two we have applied.

%Explain how Andrew's curves work, what they mean.




\section{Classification of weighted PPI networks}

\subsection{Missing data}
%how was missing data dealt with?

%Justification for mean value filling.

%Accuracy as a simple measure of the performance of a classifier is difficult to interpret in the case of a heavily unbalanced classifier such as this.
\subsection{Classifier accuracy and best parameters}

%Using grid searches over the following parameter ranges we were able to search for the optimal parameters for each of the classifiers tested.

%table of the best parameters obtained for each classifier.

\subsection{ROC curves}

%An ROC curve plots the tradeoff between true positive and false positive rates, in the case of an unbalanced classifier large sample sizes are required to obtain a smooth, stable curve.

%ROC curves for the different classifiers

%differences between the classifiers and reasons for this.

\subsection{Precision-recall curves}

%what a precision recall plot is?

%precision recall curves for the different classifiers


\subsection{Feature importances}

%Comparing logistic regression to random forests

%tests characterising and comparing different classifiers, results

%Treating interactions as an unobserved random variable, we were able to build a simple probabilistic model to make up for the failings of the classifier and continue with the Community Detection.

\section{Comparison of weighted and unweighted PPI networks}

%Here are the communities we detected in each case

%images of both sets of communities, nicely rendered

%Investigate some of the communities by eye, look at distribution of baits etc

\subsection{Graph comparison}

%comparison of using weighted and unweighted
%NMI and disease enrichment

\section*{Conclusion}


